# Llama-2-7B TrueDef ENHANCED (masking/switching NEs in the instruction)
# using Llama-2-7b chat version
base_model: meta-llama/Llama-2-7b-chat-hf
prompt_template_name: reverse_INST
# using dataset converted from MSEQA format to GenQA format (instruction, input, output) columns + masking
data_path: ./datasets/pileNER/pileNER_GenQA_format_TrueDef_enhanced/train.jsonl
val_data_path: ./datasets/pileNER/pileNER_GenQA_format_TrueDef_enhanced/validation.jsonl
select_train_portion: -1
val_set_size: -1
output_dir: ./trained_models/llama2_4_NER_TrueDef_enhanced_2
#training hyperparams
batch_size: 256
micro_batch_size: 1
num_epochs: 6
learning_rate: 3.0e-4
cutoff_len: 768

warmup_steps: 600
eval_steps: 200
logging_steps: 10
max_grad_norm: 1.0
#lora hyperparams
use_lora: True
lora_alpha: 16
lora_dropout: 0.05
lora_r: 8
lora_target_modules:
- q_proj
- v_proj
- k_proj
- v_proj
#llm hyperparams
# NTP loss only on Response
train_on_inputs: False
group_by_length: True
#quant params
load_8bit: False
load_4bit: False
#general param
save_total_limit: 5
use_flash_attention: False
shuffle: True
gradient_checkpointing: False